{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention-bidirectional_glove.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT6dF2vpw_9F"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive/Colab_Notebooks\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "metadata": {
        "id": "awENdO7WtX2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dabe2bd-cb65-4be6-a7c0-a2782b99c64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['Untitled0.ipynb', 'Copy of Untitled0.ipynb', 'Untitled2.ipynb', 'Untitled3.ipynb', 'Untitled6.ipynb', 'Copy of Untitled4.ipynb', 'LSTMimdb.ipynb', 'Untitled5.ipynb', 'BERTproductreview (1).ipynb', 'Untitled4.ipynb', 'Copy of FasterRCNN_Tutorial_MeyerA.ipynb', 'Untitled7.ipynb', 'RCNN.ipynb', 'Copy of RCNN.ipynb', 'VOC2012', 'VOC2012.zip', 'VOCfolder', '.ipynb_checkpoints', 'FRCNN.ipynb', 'yelp_review_polarity_csv.zip', 'test_1copy.csv', 'test.csv', 'train.csv', 'yelp_review_polarity_csv', 'deceptive-opinion.csv', 'EDAonYelpReviews.ipynb', 'glove.6B.100d.txt', 'BERTproductreview.ipynb', 'Cnnonfashionmnsit.ipynb', 'train_1copy.csv', 'IMDB_Dataset.csv', 'IMDB Dataset.csv', 'Attention-bidirectional_glove.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYnTm3YhqWpM",
        "outputId": "283aff49-522c-484b-ccd6-fc158cd62055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.7/dist-packages (0.26.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.26.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (0.26.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.io as tfio"
      ],
      "metadata": {
        "id": "ZVcSmcLEqaUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_laplacian(input, ksize, mode=None, constant_values=None, name=None):\n",
        "    \"\"\"\n",
        "    Apply Laplacian filter to image.\n",
        "    Args:\n",
        "      input: A 4-D (`[N, H, W, C]`) Tensor.\n",
        "      ksize: A scalar Tensor. Kernel size.\n",
        "      mode: A `string`. One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\"\n",
        "        (case-insensitive). Default \"CONSTANT\".\n",
        "      constant_values: A `scalar`, the pad value to use in \"CONSTANT\"\n",
        "        padding mode. Must be same type as input. Default 0.\n",
        "      name: A name for the operation (optional).\n",
        "    Returns:\n",
        "      A 4-D (`[N, H, W, C]`) Tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    input = tf.convert_to_tensor(input)\n",
        "    ksize = tf.convert_to_tensor(ksize)\n",
        "\n",
        "    tf.debugging.assert_none_equal(tf.math.mod(ksize, 2), 0)\n",
        "\n",
        "    ksize = tf.broadcast_to(ksize, [2])\n",
        "\n",
        "    total = ksize[0] * ksize[1]\n",
        "    index = tf.reshape(tf.range(total), ksize)\n",
        "    g = tf.where(\n",
        "        tf.math.equal(index, tf.math.floordiv(total - 1, 2)),\n",
        "        tf.cast(1 - total, input.dtype),\n",
        "        tf.cast(1, input.dtype),\n",
        "    )\n",
        "\n",
        "    input = pad(input, ksize, mode, constant_values)\n",
        "\n",
        "    channel = tf.shape(input)[-1]\n",
        "    shape = tf.concat([ksize, tf.constant([1, 1], ksize.dtype)], axis=0)\n",
        "    g = tf.reshape(g, shape)\n",
        "    shape = tf.concat([ksize, [channel], tf.constant([1], ksize.dtype)], axis=0)\n",
        "    g = tf.broadcast_to(g, shape)\n",
        "    return tf.nn.depthwise_conv2d(input, g, [1, 1, 1, 1], padding=\"VALID\")"
      ],
      "metadata": {
        "id": "RSS4LtV0ovFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/IMDB_Dataset.csv')\n",
        "#df = pd.read_csv('./deceptive-opinion.csv')\n",
        "df.head()\n",
        "\n",
        "#df = df.drop([\"hotel\", \"polarity\",\"source\"], axis=1)\n",
        "\n",
        "df.head()\n",
        "\n",
        "df1=df.sample(frac=1)\n",
        "\n",
        "df1.head()\n",
        "\n",
        "from sklearn import preprocessing \n",
        "\n",
        "# label_encoder object knows how to understand word labels. \n",
        "label_encoder = preprocessing.LabelEncoder() \n",
        "\n",
        "# Encode labels in column 'species'. \n",
        "df1['deceptive']= label_encoder.fit_transform(df1['deceptive']) \n",
        "\n",
        "df1['deceptive'].unique() \n",
        "\n",
        "df1.head()\n",
        "\n",
        "df1.describe()\n",
        "\n",
        "X = df1.text\n",
        "y = df1.deceptive\n",
        "\n",
        "# Some preprocesssing that will be common to all the text classification methods\n",
        "\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, f' {punct} ')\n",
        "    return x\n",
        "\n",
        "def clean_numbers(x):\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "        x = re.sub('[0-9]{5,}', '#####', x)\n",
        "        x = re.sub('[0-9]{4}', '####', x)\n",
        "        x = re.sub('[0-9]{3}', '###', x)\n",
        "        x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n",
        "\n",
        "df1.head()\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_data(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower().split()\n",
        "    text = \" \".join(text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    return text\n",
        "    \n",
        "df1['text'] = df1['text'].apply(clean_data)\n",
        "\n",
        "X = df1.text\n",
        "y = df1.deceptive\n",
        "\n",
        "X = X.map(lambda a: clean_data(a))\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import logging\n",
        "from numpy import random\n",
        "import gensim\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, MaxPooling1D, Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "train, test, y_train, y_test = train_test_split(X,y,stratify=y,random_state=42)\n",
        "\n",
        "train, val, y_train, y_val = train_test_split(train,y_train,stratify=y_train,random_state=42)\n",
        "\n",
        "#train, val, y_train, y_val = train_test_split(X,y,stratify=y,random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=None,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=' ',char_level=False)\n",
        "\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(train)\n",
        "\n",
        "x_test = tokenizer.texts_to_sequences(test)\n",
        "\n",
        "x_val = tokenizer.texts_to_sequences(val)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "vocab_size = len(word_index)\n",
        "print('Vocab size: {}'.format(vocab_size))\n",
        "longest = max(len(seq) for seq in X)\n",
        "print(\"Longest comment size: {}\".format(longest))\n",
        "average = np.mean([len(seq) for seq in X])\n",
        "print(\"Average comment size: {}\".format(average))\n",
        "stdev = np.std([len(seq) for seq in X])\n",
        "print(\"Stdev of comment size: {}\".format(stdev))\n",
        "max_len = int(average + stdev * 3)\n",
        "print('Max comment size: {}'.format(max_len))\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "processed_post_x_train = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')\n",
        "processed_post_x_test = pad_sequences(x_test, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "processed_post_x_val = pad_sequences(x_val, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "processed_x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "processed_x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "processed_x_val = pad_sequences(x_val, maxlen=max_len)\n",
        "\n",
        "processed_pre_x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "processed_pre_x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "processed_pre_x_val = pad_sequences(x_val, maxlen=max_len)\n",
        "\n",
        "\n",
        "print('x_train shape:', processed_x_train.shape)\n",
        "print('x_test shape:', processed_x_test.shape)\n",
        "\n",
        "#from tensorflow.keras.engine.topology import Layer\n",
        "from tensorflow.keras.layers import Layer    \n",
        "\n",
        "import keras.backend\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import CuDNNGRU, Dense, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization, LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/drive/MyDrive/Colab_Notebooks', 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 100\n",
        "k = 0\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        k += 1\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "from scipy import ndimage, misc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        #self.init = initializations.get('glorot_uniform')\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
        "\n",
        "        # features_dim = self.W.shape[0]\n",
        "        # step_dim = x._keras_shape[1]\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        #print(type(a))\n",
        "        #print(\"Shape is : \",a.get_shape)\n",
        "        #a=tf.reshape(a,[1,None,415,120])\n",
        "        a = K.expand_dims(a)\n",
        "        #print(\"Shape is : \",a.get_shape)\n",
        "        print(a.shape)\n",
        "\n",
        "\n",
        "        a=tfio.experimental.filter.laplacian(a,5)\n",
        "        \n",
        "        a=K.squeeze(a,3)\n",
        "\n",
        "\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        x = K.expand_dims(x)\n",
        "        \n",
        "        x=tfio.experimental.filter.laplacian(x,5)\n",
        "        \n",
        "        #a = tf.reshape(a,[None,415,120])\n",
        "        x =K.squeeze(x,3)\n",
        "\n",
        "        x = K.expand_dims(x)\n",
        "        \n",
        "        x=tfio.experimental.filter.laplacian(x,5)\n",
        "        \n",
        "        #a = tf.reshape(a,[None,415,120])\n",
        "        x =K.squeeze(x,3)\n",
        "\n",
        "        x = K.expand_dims(x)\n",
        "        \n",
        "        #x=tfio.experimental.filter.laplacian(x,5)\n",
        "        \n",
        "        #a = tf.reshape(a,[None,415,120])\n",
        "        x =K.squeeze(x,3)\n",
        "        weighted_input = x * a\n",
        "    #print weigthted_input.shape\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        #return input_shape[0], input_shape[-1]\n",
        "        return input_shape[0],  self.features_dim\n",
        "\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, concatenate\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "import keras.backend\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import CuDNNGRU, Dense, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization, LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    review_input = Input(shape=(max_len,), dtype='int32')\n",
        "    #review_input_post = Input(shape=(max_len,), dtype='int32')\n",
        "    \n",
        "    \n",
        "    x1 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input)\n",
        "    x1 = Bidirectional(LSTM(60, return_sequences=True))(x1)\n",
        "    x1 = Dropout(0.3)(x1)\n",
        "    x1 = Attention(max_len)(x1)\n",
        "    \"\"\"\n",
        "    x2 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input_post)\n",
        "    x2 = Bidirectional(LSTM(60, return_sequences=True))(x2)\n",
        "    x2 = Dropout(0.3)(x2)\n",
        "    x2 = Attention(max_len)(x2)\n",
        "    \"\"\"\n",
        "    x = x1 # concatenate([x1, x2])\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "    x= Dropout(0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    preds = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=[review_input], outputs=preds)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "review_input = Input(shape=(max_len,), dtype='int32')\n",
        "review_input_post = Input(shape=(max_len,), dtype='int32')\n",
        "    #comment_input=Input(dtype='int32')\n",
        "    #comment_input_post=Input(dtype='int32')\n",
        "    #embedding_matrix=torch.Tensor(embedding_matrix)\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size + 1, embedding_dim, weights=[torch.Tensor(embedding_matrix)], input_length=max_len, trainable=True))#(comment_input)\n",
        "model.add(Bidirectional(LSTM(60, return_sequences=True)))#(x1)\n",
        "model.add(Dropout(0.3))#(x1)\n",
        "model.add(Attention(embedding_dim))#(x1)\n",
        "\n",
        "model.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True))#(comment_input_post)\n",
        "model.add(Bidirectional(LSTM(60, return_sequences=True)))#(x2)\n",
        "model.add(Dropout(0.3))#(x2)\n",
        "model.add(Attention(input_length))#(x2)\n",
        "\n",
        "    #x = concatenate([x1, x2])\n",
        "model.add(Dense(50, activation='relu'))#(x)\n",
        "model.add(Dropout(0.2))#(x)\n",
        "model.add(BatchNormalization())#(x)\n",
        "preds = Dense(1, activation='sigmoid')#(x)\n",
        "model = Model(inputs=[review_input, review_input_post], outputs=preds)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping_monitor = EarlyStopping(patience=10)\n",
        "\n",
        "history = model.fit(processed_x_train,y_train, validation_data=(processed_x_val,y_val), epochs=10,batch_size=32)#,callbacks=[early_stopping_monitor])\n",
        "                    \n",
        "model.evaluate(processed_x_test,y_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NzzFWEdwJX4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_x_train[0])\n",
        "print(processed_pre_x_train[0])\n"
      ],
      "metadata": {
        "id": "7Ld1yc4w2n94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classified=model.predict(processed_x_test)\n",
        "classified=np.concatenate(classified,axis=0)\n",
        "misclassified=(np.round(classified)!= y_test )\n",
        "#print(misclassified)\n",
        "dfm=pd.DataFrame()\n",
        "dfm['m']=misclassified\n",
        "#print(dfm)\n",
        "i=dfm.index[dfm['m'] == True].tolist()\n",
        "print(i)\n",
        "print(len(i))\n",
        "print(len(processed_x_test))\n",
        "for j in i:\n",
        "    print(df.iloc[j,:])"
      ],
      "metadata": {
        "id": "6tqw2855iJhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HeUNk48Lga1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}